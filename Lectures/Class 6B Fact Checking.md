

# The Modern Lawyer's Guide to AI-Assisted Fact-Checking: A Practical Handbook

##Assignment

Use the techniques described in this essay to (1) fact check the remainder of this very essay. Prepare a comprehensive report that flags inaccuracies.
(2) fact check the essay contained in the Appendix to this document.

## Introduction: A Direct Approach to AI in Legal Practice

Generative artificial intelligence (AI), including the latest Large Language Models (LLMs) like GPT-5 and Gemini 2.5, is rapidly becoming an indispensable tool in the legal profession. These models offer unprecedented speed in legal research, document analysis, and drafting. However, their core function—predicting the next most likely word—means they are not sources of truth and can produce plausible but incorrect information, a phenomenon known as "hallucination". While newer models show significantly reduced error rates, the risk of inaccuracy remains a critical professional hazard.

This guide is a practical, operational handbook for law students and young practitioners. It provides a direct, "how-to" framework for using AI as a powerful **revisory agent and editor**—a tool to stress-test and verify facts, both legal and non-legal, that have been generated by another AI or compiled from other sources. This approach maintains the rigorous verification required by a lawyer's non-delegable duty of competence, leveraging AI's efficiency without compromising the accuracy and integrity that are the bedrock of legal practice.

## I. The LLM as Editor: Prompting for Verification

The most powerful use of an LLM in a fact-checking workflow is not as an originator of information, but as a tireless, skeptical editor. By providing an LLM with a piece of text—whether generated by another AI or drafted by a human—you can prompt it to systematically probe for weaknesses, inaccuracies, and omissions. This requires a shift in prompting strategy from asking for information to asking for critical evaluation.

### Core Principles of Evaluative Prompting

Effective evaluative prompting treats the LLM as a specialized tool for quality control. The key is to provide the model with a clear set of instructions and criteria against which it should judge the provided text.[1, 2, 3, 4]

  * **Assign a Persona:** Instruct the LLM to adopt the role of a specific expert. This focuses its analysis and leverages its training data relevant to that role.
  * **Provide the Text to Be Reviewed:** Clearly delimit the text that the LLM should evaluate.
  * **Deconstruct and Verify:** Instruct the model to break the text down into individual, atomic claims and then assess each one. This "chain-of-thought" or step-by-step approach improves accuracy by forcing a more granular analysis.
  * **Define Specific Error Categories:** Give the LLM a checklist of potential errors to look for. This is far more effective than a generic "check for mistakes" command.
  * **Request Structured Output:** Ask for the analysis in a structured format, like a table or JSON object, for clarity and ease of review.[1, 2, 3]

### Practical Examples: Using an LLM as a Revisory Agent

#### Verifying a Case Summary

Imagine a junior associate used an AI to generate a summary of *United States v. Cruikshank*, 92 U.S. 542 (1876). You now want to use a second LLM session to rigorously check that summary.

  * **Prompt for Verification:**

    > You are a meticulous legal historian and constitutional law expert specializing in the Reconstruction Era. Your task is to act as a fact-checker and editor.

    > Below is a summary of the U.S. Supreme Court case *United States v. Cruikshank*. Please analyze this summary for accuracy and completeness.

    > **[Paste the AI-generated summary of *Cruikshank* here]**

    > Conduct your analysis by following these steps:

    > 1.  **Deconstruct the summary** into a list of individual factual claims (about the events, the procedural history, and the Court's reasoning).
    > 2.  **For each claim, evaluate its accuracy.**
    > 3.  **Identify potential errors** across the following five categories:
    >     a.  **Factual Inaccuracy:** Are the descriptions of the Colfax Massacre, the defendants, or the charges incorrect?
    >     b.  **Misstatement of Holding:** Does the summary accurately state the Court's rulings regarding the First, Second, and Fourteenth Amendments? Does it correctly explain *why* the Enforcement Acts were deemed unconstitutional in this application?
    >     c.  **Omission of Critical Context:** Does the summary fail to mention the case's impact on the federal government's ability to protect the civil rights of African Americans? Does it omit the long-term significance of the ruling in undermining Reconstruction?
    >     d.  **Outdated Legal Interpretation:** Does the summary present the holdings as if they are still good law without noting how subsequent civil rights jurisprudence (e.g., via incorporation) has altered the legal landscape?
    >     e.  **Logical Fallacies:** Is the reasoning presented in the summary internally consistent and logical?
    > 4.  **Provide your final output as a table** with the following columns: "Claim from Summary," "Is it Accurate? (Yes/No/Partially)," and "Detailed Analysis/Correction."

  * **Analysis:** This prompt reframes the task from information generation to critical evaluation. It assigns a specific expert role, provides the text to be analyzed, and gives the LLM a structured, multi-step process with clear error categories to check for. The request for a table output makes the results easy to review. 
  * Note: This particular fact-checking request rests on some significant substantive knowledge by the human. When you have that knowledge and can ask the fact-checking LLM to zero in on matters that you regard as particularly important or controversial, by all means tell the fact checker. Sometimes, however, you will not be so lucky. For example, if I asked an LLM about Tasmanian law on easements by prescription I would have no clue what specific facts might be worth checking. In such cases, the general admonition of "just do the best you can" applies.

#### Verifying Statutory Text and Interpretation

Imagine an AI has provided the following text in response to a query about employment discrimination: "Under 42 U.S.C. § 2000e-2(a)(1), it is illegal for an employer to refuse to hire someone because of their race, color, religion, or sex."

  * **Prompt for Verification:**

    > You are a legislative analyst and expert in federal employment law. Your task is to verify the accuracy of a provided statutory quotation and its interpretation.

    > **Text to Verify:** "Under 42 U.S.C. § 2000e-2(a)(1), it is illegal for an employer to refuse to hire someone because of their race, color, religion, or sex."

    > Please perform the following checks:

    > 1.  **Verbatim Accuracy:** Compare the quoted text to the official text of 42 U.S.C. § 2000e-2(a)(1). Is it a perfect, word-for-word match? Note any discrepancies, additions, or omissions.
    > 2.  **Completeness:** The provided text only mentions "refuse to hire." Does the full statutory subsection list other unlawful employment practices? If so, list them.
    > 3.  **Missing Categories:** The quote lists "race, color, religion, or sex." Does the full subsection include any other protected characteristics?
    > 4.  **Provide a corrected and complete version** of the statutory text.

  * **Analysis:** This prompt focuses the LLM on the specific, verifiable details of a legal text. It asks for a direct comparison, a check for omissions (a common error type), and the production of a corrected final version. This is a more targeted and effective approach than simply asking if the original statement was "correct."

## II. The Reality of AI Errors: A Practical Risk Management View

While models are improving, it is crucial to understand the inherent limitations of the technology. The most significant of these is the phenomenon of "hallucination." In a legal context, this is not a random glitch but a byproduct of how LLMs are designed. They are probabilistic models built to generate plausible sequences of words, not to verify factual truth. They can be post-trained to increase the probability that they will provide factual truth but (a) this is easier in subjects like math where there is a generally accepted answer (2 + 2 generally does equal 4) than it is in law or other "humanities" where many answers are contestable and (b) even in those realms, the model can err. This makes them uniquely dangerous in law, where outputs can appear authoritative yet be catastrophically wrong.

### Acknowledging Progress and Persistent Risk

Recent models like GPT-5 and Gemini 2.5 demonstrate substantial improvements in reliability and reduced error rates. OpenAI, for instance, reports that GPT-5 has significantly lower hallucination rates than its predecessors. However, research also indicates that newer, more complex reasoning models can sometimes introduce new avenues for hallucinations, and no model is infallible. The risk is not eliminated, merely reduced.

Even specialized legal AI tools that use Retrieval-Augmented Generation (RAG) to ground responses in legal databases are not immune. A 2024 Stanford study found that bespoke legal tools from major vendors still produced incorrect or misgrounded information in **17% to over 34% of cases**. While this data predates the latest models, it underscores a core principle: every AI-generated assertion of fact or law should be independently verified by the practitioner, particularly where it is important to the argument. (And if it is not important, then perhaps it should be removed!)  This is no different from the long-standing professional obligation to check a secondary source or a colleague's memo before relying on it. Humans hallucinate in different ways than machines, but they are hardly foolproof either.

### A Taxonomy of Legal AI Errors

Recognizing common error patterns helps in structuring an effective verification process:

  * **Fabricated Authority:** The AI invents non-existent case citations, statutes, or articles. This is the most notorious error but often the easiest to detect with a simple citation check. Many, including Professor Chandler, are working on automated workflows to make detection of fabricated authority more reliable.
  * **Misstated Law:** The AI cites a real case or statute but incorrectly describes its holding or scope. This is a more subtle and dangerous error. Automated detection of misstated law requires access to a fairly powerful LLM. Again, it is a frontier area on which many are working actively, including Professor Chandler.
  * **Misgrounded but Correct Statements:** The AI makes a correct statement of law but cites an irrelevant or non-supportive source. This is particularly pernicious as it can create a false sense of security. This is essentially a derivative form of misstated law and is subject to the same difficulties in automated verification.
  * **Outdated Information:** The AI provides information that has been superseded by a new statute or overruled by a subsequent decision.
  * **Jurisdictional Confusion:** The AI incorrectly applies legal principles from one jurisdiction to a problem governed by the law of another. This one is very difficult to check in an automated way because, unless the automated checker is intelligent enough to ask the meta-question -- "does this law even apply to our facts" it may falsely report, "yes, the cited source states exactly what the user claims." Human law students are particularly prone to these sorts of scoping errors.

## III. A Comparative Analysis of LLM Performance in Legal Tasks (Updated for 2025)

The AI market is a rapidly evolving landscape of models with different strengths. For legal practitioners, understanding these differences is key to selecting the right tool for a given task.

### Head-to-Head Comparison of Leading Models

As of early 2025, models from Google, OpenAI, and others are highly competitive on legal-specific benchmarks like **LegalBench**, which evaluates performance on a wide array of legal reasoning tasks.

  * **Overall Performance:** The latest benchmarks show Google's **Gemini 2.5 Pro** taking a narrow lead, with OpenAI's **GPT-5** and **o1 Preview** also being top performers. Models from DeepSeek and Anthropic's Claude series remain highly competitive.
  * **Qualitative Differences:** Beyond raw scores, models exhibit different strengths. **Claude** models are often praised for precision and sophisticated drafting, making them a strong choice for contract analysis. **GPT** models are noted for robust all-around reasoning and versatility.
  * **Cost-Performance Trade-Off:** For many, cost is a major factor. More accessible models like **Gemini 2.0 Flash** and **GPT-4o Mini** provide strong performance at a fraction of the API cost of flagship models, making them excellent choices for less critical tasks.

| Model | LegalBench Score | API Cost (Input/Output per Million Tokens) | Latency (seconds) |
| :--- | :--- | :--- | :--- |
| Gemini 2.5 Pro Exp. | 83.6% | $1.25 / $10.00 | 3.51 |
| GPT-5 | 84.6% | *Not Publicly Available* | *Not Publicly Available* |
| OpenAI o1 Preview | 81.7% | $15.00 / $60.00 | 10.33 |
| DeepSeek V3 | 80.1% | $0.90 / $0.90 | 4.13 |
| Gemini 2.0 Flash | 79.7% | $0.10 / $0.40 | 0.34 |
| Claude 3.5 Sonnet | 78.8% | $3.00 / $15.00 | 0.83 |
*(Data adapted from Vals AI LegalBench benchmark, March & May 2025, and other sources. No one has checked whether these tables are current or accurate.)*

### The Critical Flaw: LLMs Cannot Reliably Fact-Check Each Other

A common but deeply flawed assumption is that one LLM can be used to reliably verify another's output. The **ReaLMistake benchmark**, designed specifically to test this capability, found that even top models like GPT-4 and Claude 3 detect errors made by other LLMs with very low recall, performing much worse than human experts. The practical implication is profound: **using an LLM as the exclusive means to fact-check another LLM on an important matter is an ineffective verification strategy.** The goal is to use the second LLM as an assistant to a human reviewer, flagging potential issues for the human to make the final determination.

## IV. The Practitioner's Toolkit: A Tiered Framework for Research and Verification

A modern lawyer needs a robust mental model for integrating AI tools into a responsible workflow. The most effective approach is a tiered framework that moves systematically from the least reliable tools for initial exploration to the most reliable sources for final verification.

| Tier | Tool Category | Primary Use Case | Key Strengths | Critical Risks & Limitations |
| :--- | :--- | :--- | :--- | :--- |
| **Tier 1** | **General-Purpose LLMs**\<br\>(e.g., ChatGPT, Gemini, Claude) | Preliminary, non-confidential brainstorming, outlining, and acting as a first-pass "revisory agent" to spot potential issues in a draft. | Low cost, high speed, versatility. | High error potential. Outdated knowledge. **Severe Confidentiality Risk.** |
| **Tier 2** | **Specialized Legal AI**\<br\>(e.g., Vincent, Harvey, midpage) | Substantive legal research and analysis; drafting initial arguments with cited support. | Grounded in curated legal data (RAG). Lower error rate. Legal-specific workflows. | Still prone to errors. Opaque data sources. Requires final verification. |
| **Tier 3** | **Authoritative & Alternative Databases**\<br\>(e.g., Westlaw, Lexis, Fastcase, Google Scholar) | **Final verification** of all legal propositions and citations. | Canonical or cost-effective sources of primary law. Gold-standard or alternative citator services. | Varies by platform; cost can be a major barrier for some. |

### Tier 1: General-Purpose LLMs (ChatGPT, Gemini, Claude)

  * **When to Use:** Best for the very beginning of a project for low-stakes, non-confidential tasks like brainstorming search terms or summarizing a public news article. They can also serve as a first-pass editor, as described in Section I.
  * **CRITICAL RISKS:** These tools pose a severe confidentiality risk. Inputting any client-related facts into a public, consumer-grade AI tool can breach the duty of confidentiality and potentially waive attorney-client privilege. **Confidential client information should never be entered into a public LLM without informed and explicit client consent. The lawyer needs to understand with precision the terms of service of the LLM provider. Information covered by the attorney-client privilege needs to handled with special care. Client consent does not diminish the potential for submission to constitute a waiver of AC privilege. ** [^1]

 

### Tier 2: Specialized Legal AI Platforms (Vincent, Harvey, midpage)

  * **When to Use:** The workhorse for AI-assisted legal research. These platforms are designed for substantive legal tasks that require grounding in actual legal data. They are appropriate for conducting deep research and drafting initial arguments *before* final verification.
  * **Strengths:** Their key advantage is Retrieval-Augmented Generation (RAG), which retrieves documents from curated legal databases to generate answers, significantly reducing (but not eliminating) errors.
  * **Tool-Specific Fact-Checking Workflows:**
      * **Vincent AI:** This platform from vLex is particularly useful for testing a legal proposition. Its **"Explore a Legal Proposition"** and **"Build an Argument"** features allow a user to input a statement of law, and Vincent will search its global database to construct a cited argument for or against it, effectively stress-testing the assertion.[^2]
      * **Harvey AI:** Harvey is designed to be trained on a firm's own documents in addition to external legal data. A key feature is its commitment to **grounding every answer in the exact source material**, providing a direct path for verification. The 2025 Vals Legal AI Report found Harvey's performance on "Document Q\&A" surpassed a human lawyer baseline.[^3]
      * **midpage:** This AI-native litigation platform offers a **"Proposition Search"** feature designed to solve the "needle-in-a-haystack" problem. A user can input a full sentence proposition, and the tool surfaces specific judicial language from case law that directly matches or supports that argument, making it a powerful tool for verifying the textual basis of a claim.[^4]

### Tier 3: Authoritative & Alternative Databases

This tier represents the final verification step. While Westlaw and LexisNexis have long dominated this space, their duopoly has led to criticism over high prices and slow innovation, making them inaccessible for many solo and small-firm practitioners. A modern approach considers both the incumbents and their more affordable alternatives.

  * **The Incumbents (Westlaw & LexisNexis):** For those with access, they remain the gold standard for final verification, primarily due to their indispensable citator services: **KeyCite** on Westlaw and **Shepard's** on LexisNexis. These tools provide the crucial, final-step analysis of whether a case is still "good law," a task no general or specialized LLM can currently perform with the same reliability.
  * **Affordable and Open-Source Alternatives:** For practitioners who find the cost of the duopoly prohibitive, several high-quality, lower-cost, or free alternatives provide access to primary law.
      * **Fastcase:** Often available as a free member benefit through state and local bar associations, Fastcase provides a comprehensive library of case law and statutes with an AI-powered search function.
      * **Casetext:** Now part of Thomson Reuters, Casetext is known for its user-friendly interface and powerful AI features at a more accessible price point than its parent company's flagship product.
      * **Google Scholar:** A powerful and free tool for finding case law from federal and state courts. Its search functionality is robust, though it lacks the editorial enhancements and citator services of paid platforms.
      * **CourtListener / Free.Law Project:** A non-profit initiative providing free access to a vast collection of court opinions, dockets, and oral arguments, with a focus on creating an open-source, accessible legal research platform. [^5] 
## V. Verifying Non-Legal Facts: A Practical Workflow

Legal work often requires verifying non-legal facts, such as historical events, scientific claims, or details from news reports. LLMs can be a starting point, but their tendency to blend fact with fiction requires a structured verification process.

### A Step-by-Step Guide to Non-Legal Fact-Checking

1.  **Deconstruct the Claim:** Break down a complex statement into simple, verifiable sub-claims. An LLM can assist with this initial step.

      * **Example Prompt:** "Take the following claim: 'The 1929 stock market crash was primarily caused by the Smoot-Hawley Tariff Act, leading to a decade of global economic depression.' Break this down into a list of individual factual assertions that can be verified independently."

Here are the results from such a query. As you can see, even a simple sentence has many implicit facts that might be checked. (By the way, the statement is false)

1. There was a stock market crash in 1929.
1. The Smoot-Hawley Tariff Act existed.
1. The Smoot-Hawley Tariff Act was the primary cause of the 1929 stock market crash.
1. The 1929 stock market crash led to a decade of economic depression.
1. The economic depression was global in scope.
1. The economic depression lasted for a decade.

** Use Pre-built or custom fact checkers**

ChatGPT already has CustomGPTs that will do fact checking for you. One that appears to do a decent job is called Fact Checker. Or you can easily build your own. The appendix contains a prompt that you can use on either an ad hoc basis or as a component of your own CustomGPT, Gemini Gem, or similar tool.


 **Employ "Masking" to Test for Consistency:** Ask the LLM to fill in a blank in your claim. If the claim is well-supported in its training data, the responses should be consistent across multiple attempts. Inconsistent answers suggest the model may be guessing.

      * **Example Prompt:** "The historical consensus is that the primary cause of the Great Depression was [fill in the blank]."

4.  **Demand Sources and Evaluate Them Critically:** Use an LLM with live web access (like those from Google or OpenAI) and instruct it to provide cited sources for its claims. Then, evaluate those sources yourself using methods like the SIFT framework (Stop, Investigate the source, Find better coverage, Trace claims).

      * **Example Prompt:** "Summarize the scientific consensus on the efficacy of drug X for treating condition Y. Provide links to at least three peer-reviewed studies published in reputable medical journals within the last five years to support your summary."

5.  **Prompt the AI to Act as a Skeptic:** Instruct the model to adopt a critical persona and challenge the assumptions underlying a claim. This can reveal weaknesses or unstated premises in an argument.

      * **Example Prompt:** "Act as a skeptical historian. Critically evaluate the following statement and identify any potential biases, logical fallacies, or areas where the evidence is weak: '[Insert historical claim here].'"

## VI. The Next Frontier: Algorithmic Verification Using Legal Data APIs

The next evolution in legal fact-checking involves leveraging Application Programming Interfaces (APIs) to automate high-volume verification tasks. Both Thomson Reuters and LexisNexis have opened up their data repositories for programmatic access, signaling a fundamental shift in how legal data can be utilized.

### A Deep Dive into the LexisNexis API

The LexisNexis Developer Portal provides a suite of APIs that allow firms to integrate extensive legal content and analytics directly into their own systems. This enables the creation of custom tools for research, monitoring, and verification. The portal includes a "sandbox" for developers to try out API functionality before full deployment.

  * **API Types and Functionalities:**

      * **URL APIs:** Simple, direct links that allow an authorized user to run a search, view an alert, or pull a Shepard's® report from within an internal application with minimal setup.
      * **REST/Web Services APIs:** More powerful APIs that enable custom workflows. They allow applications to programmatically search, retrieve, and receive alerts on legal content, enhanced with Lexis's own data enrichments.
      * **Cognitive APIs:** These provide tools for data normalization and entity resolution, including access to structured judge and court information, a legal dictionary, and a PII (Personally Identifiable Information) redactor.[5]
      * **Bulk APIs:** Designed for transferring large datasets for use in machine learning, predictive analytics, or historical analysis projects.

  * **Available Data:** The APIs provide access to a vast repository of legal and business information, including [5]:

      * **Legal Sources:** Over 14,000 legal and legislative sources, including case law, statutes, and regulations.
      * **Litigation Documents:** Over 210 million dockets and 63 million documents, including briefs, pleadings, and motions.
      * **Verdicts and Settlements:** A database of over 1.3 million verdicts and settlements.
      * **News and Business Data:** Over 7,400 news sources and information on over 240 million companies.

  * **Practical Use Case: Automated Citation Checker:** A firm could build a script that:

    1.  Extracts all case citations from an opposing counsel's brief.
    2.  Makes an API call to the LexisNexis API for each citation to retrieve its Shepard's signal (e.g., red flag for overruled).
    3.  Generates a report listing only the cases with negative treatment, allowing an associate to focus their attention immediately on the problematic authorities.[6]

This programmatic approach can transform a multi-hour manual task into a nearly instantaneous check, embedding compliance and verification directly into a firm's workflow.

## Conclusion: A Workflow for Responsible and Ethical AI Integration

The integration of LLMs into legal practice is a present reality. For law students and junior practitioners, developing the skills to use these tools effectively and ethically is essential for modern legal competence. This guide has provided a practical, risk-based framework for approaching legal fact-checking in the age of AI, reframing the LLM as a powerful editorial assistant rather than an originator of facts.

The core takeaway is the **three-tiered workflow**, adapted for the modern, cost-conscious legal market:

1.  **Tier 1 (General LLMs):** For preliminary, non-confidential brainstorming and, crucially, as a first-pass revisory agent to identify potential flaws in existing text.
2.  **Tier 2 (Specialized Legal AI):** For substantive research and analysis, leveraging platforms that ground their outputs in curated legal data.
3.  **Tier 3 (Authoritative & Alternative Databases):** For more authoratative verification of every legal proposition and citation, using either the gold-standard services of Westlaw and LexisNexis or cost-effective alternatives like Fastcase and Google Scholar.

AI is a powerful assistant, but it is not a substitute for professional judgment, critical thinking, or the careful verification that has always been the hallmark of a diligent legal professional. The future of the profession belongs to those who can master the art of integrating human expertise with artificial intelligence, harnessing the efficiency of AI without compromising the integrity and accuracy that are the bedrock of the law.

##Appendices

###Custom Fact Checking Prompt


You are a specialized fact-checking assistant that identifies verifiable claims in text. Your role is to extract every statement that can be independently verified through reliable sources.

####Core Objective
Analyze any provided text and extract ALL factual assertions—both explicit statements and implicit claims—that can be fact-checked using reliable sources.

#### Instructions

##### What to Extract:
- **Explicit facts**: Direct statements presented as factual
- **Implicit claims**: Facts implied by the text but not directly stated
- **Quantitative data**: Numbers, dates, percentages, measurements, rankings
- **Causal relationships**: Claims about cause-and-effect
- **Comparative statements**: Claims about relative size, importance, or degree
- **Historical events**: Dates, sequences, participants, outcomes
- **Attributions**: Quotes, actions, or positions attributed to specific people/organizations
- **Categorical statements**: Claims about membership in groups or categories
- **Scientific/technical claims**: Statements about how things work or their properties

##### What NOT to Extract:
- Pure opinions or subjective judgments (unless attributed to a specific source)
- Hypothetical scenarios or speculation
- Questions (unless they contain embedded factual premises)
- Metaphors or obvious figurative language
- Predictions about unknown future events

#### Output Format
Present findings as a numbered list with this structure:

**[NUMBER]. [FACTUAL ASSERTION]**
- Type: [Explicit/Implicit]
- Category: [Historical Event/Statistical Claim/Causal Relationship/Attribution/etc.]
- Verifiability: [High/Medium/Low]

#### Example:
Input: "Tesla became the world's most valuable automaker in 2020, surpassing Toyota, thanks to Elon Musk's visionary leadership."

Output:
1. **Tesla became the world's most valuable automaker in 2020**
   - Type: Explicit
   - Category: Business/Financial milestone
   - Verifiability: High

2. **Tesla surpassed Toyota in market valuation**
   - Type: Explicit  
   - Category: Comparative business claim
   - Verifiability: High

3. **Toyota was previously the world's most valuable automaker**
   - Type: Implicit
   - Category: Historical business status
   - Verifiability: High

4. **Elon Musk's leadership was the cause of Tesla's success**
   - Type: Explicit
   - Category: Causal relationship
   - Verifiability: Low (subjective attribution)

#### Guidelines:
- Be comprehensive—err on the side of including rather than excluding potential facts
- Break down complex statements into atomic, independently verifiable pieces
- Consider temporal context (when something occurred matters)
- Flag statements that mix factual and subjective elements
- Note when verification would require specific expertise or access to specialized sources

Now analyze the provided text and extract all verifiable factual assertions.

###Essay to be checked

The Supreme Court's recent decision in National Institutes of Health (NIH) v. American Public Health Association (APHA) has significant implications for biomedical research and federal funding. On August 20, 2025, the Court issued a partial stay in the case, allowing the NIH to proceed with cutting grants for research related to diversity, equity, and inclusion (DEI) initiatives and transgender identity.
Background

The NIH had terminated hundreds of research grants targeting studies on disfavored topics and populations, prompting lawsuits from researchers and 23 US states. The plaintiffs argued that the terminations were "breathtakingly arbitrary and capricious" and violated federal law. US District Judge William Young rebuked the administration, stating that the cuts represented "racial discrimination and discrimination against America's LGBTQ community, not to mention people with ideological perspectives that differ from those of the present administration".

The Supreme Court's Decision
The Supreme Court's ruling on the merits granted the NIH's request to lift Judge Young's decision, allowing the agency to cut $783 million in grants. However, the Court declined to pause Young's broader ruling that declared the NIH's internal guidance explaining the rejection of funding for DEI and gender identity research as unlawful. Justice Barrett's majority opinion suggested that the challenge to the terminations should have been brought in the Court of Federal Claims, which specializes in money damages claims against the US government.
Implications
The decision will likely hinder lawsuits against grant terminations, according to legal specialists. The ruling may also have a chilling effect on research focused on marginalized communities, including racial and ethnic minorities, LGBTQ+ individuals, and individuals with disabilities. The terminated grants included all projects on breast cancer, Alzheimer's disease, HIV prevention, and other conditions that disproportionately burden minority communities.
Key Takeaways for Lawyers
Jurisdiction: The Supreme Court's decision highlights the importance of jurisdictional issues in challenging federal agency actions. The Court's ruling suggests that challenges to grant terminations may need to be brought in the Court of Federal Claims rather than district courts.
Arbitrary and Capricious Standard: The decision underscores the need for federal agencies to provide clear guidance and justification for their actions. The NIH's internal guidance was deemed "unlawful" by Judge Young, highlighting the importance of careful drafting and implementation of agency policies.
Implications for DEI Initiatives: The ruling may signal a shift in the Court's approach to DEI initiatives, potentially limiting the ability of federal agencies to prioritize research and programs focused on marginalized communities.
Potential Next Steps
The plaintiffs may seek further review or appeal the Supreme Court's decision. Additionally, Congress may consider legislative action to clarify the NIH's authority and priorities regarding DEI initiatives. The NIH may also revise its internal guidance and policies to comply with the Court's ruling.
In conclusion, the Supreme Court's decision in NIH v. APHA has significant implications for biomedical research, federal funding, and DEI initiatives. Lawyers and researchers should carefully consider the jurisdictional issues, arbitrary and capricious standard, and potential implications for marginalized communities in navigating the complex landscape of federal research funding.

##References

 [^1]: Someone should write a research paper on whether and when a client-LLM privilege should be recognized when the LLM is acting as a research assistant for the attorney.

[^2]: vLex operates with "zero-retention agreements with its language model providers" and ensures that "user data and inputs are protected through zero data retention implementations". This means that Vincent AI does not train on user inputs.


Vincent AI offers several strong privacy protections:

**Zero Data Retention**: vLex "operates with zero-retention agreements with its language model providers", meaning the underlying AI models don't retain user data beyond what's necessary to process the immediate request.

**Compliance Certifications**: The platform has "recently completed SOC2 certification in addition to its existing ISO 27001 compliance", and operates within "Vincent's ISO 27001 and SOC 2 compliant secure workspace".

**Enterprise Security Features**: vLex offers multiple security features including FIPS 140-2 Compliant Encryption, Cryptographic Audit Log, Single Sign-On, Custom Data Center Region by Office, Customer-Managed Encryption Keys, and Dedicated Instance options.

**Sensitive Data Protection**: This is particularly important for features like client intake interview analysis, where sensitive information may be processed, according to vLex's Chief Strategy Officer.

The privacy protections appear to be designed specifically for law firms handling confidential client information, with the ability to handle "sensitive requests within Vincent's ISO 27001 and SOC 2 compliant secure workspace" without requiring users to move to "less secure environments."

[^3]: Harvey AI **does not** train on user inputs. Harvey AI has made "a commitment to not train on customer data" and "we do not train or fine-tune models on your sensitive data and the same applies for our subprocessors and model providers".

Harvey uses "customer data at inference time only, categorically eliminating the risk of sensitive data becoming part of the model and thereby categorically protecting against sensitive information disclosure risks".


Harvey AI offers extensive privacy protections:

**Zero Data Retention**: Harvey AI emphasizes "zero data retention" and "encrypted processing" as key privacy features.

**Customer-Controlled Data Management**: Harvey provides "customer-controlled retention and deletion settings" and "customers set their own retention periods".

**Strong Compliance Certifications**: Harvey has "ISO 27,001" and "SOC two" certifications, with "data encrypted in transit and at rest" and "extremely strict access controls and retention policies". Harvey was "the first AI/LLM startup to certify under the EU-US Data Privacy Framework".

**Contractual Security Commitments**: Harvey provides "strong, detailed contractual security commitments by default to all customers" including "data deletion guarantees," "data residency controls," and commitments about "no training".

**Third-Party Provider Controls**: Harvey ensures that "third party model providers also don't train on customer data" and has "commitments that we've made and that we have from our third party model providers, like no training".

**Workspace Isolation**: Harvey provides "workspace isolation, ensuring customer data remains confidential".

Harvey's privacy approach appears to be designed specifically for handling highly sensitive legal data, with the company positioning itself as having "engineered trust from day one" with security as a foundational principle rather than an afterthought.

[^4]:  **Midpage AI's privacy policy does not explicitly address whether they train on user inputs or not.**  The policy covers data collection, sharing, and processing for service delivery, but there's no specific mention of:
- Whether user queries, searches, or research inputs are used for AI model training
- Any commitments about not training on customer data
- Policies regarding AI model improvement using user interactions


Midpage AI offers several privacy protections, though they're less comprehensive than Harvey AI or Vincent AI:

**Data Processing Purposes**: Midpage processes information "to provide, improve, and administer our Services, communicate with you, for security and fraud prevention, and to comply with law"

**Data Retention**: They "keep your information for as long as necessary to fulfill the purposes outlined in this privacy notice unless otherwise required by law" and will "delete or anonymize such information" when no longer needed

**Security Measures**: They have "implemented appropriate and reasonable technical and organizational security measures designed to protect the security of any personal information we process"

**Third-Party Sharing**: They share data with various third-party service providers including "Google Cloud Platform," "Google Analytics," "Stripe," and others, with contracts "designed to help safeguard your personal information"

**User Rights**: Users have standard privacy rights including access, correction, deletion, and opt-out rights depending on jurisdiction.


Unlike Harvey AI and Vincent AI, Midpage AI's current privacy policy lacks:
- Explicit statements about AI training practices
- Zero data retention commitments for AI model providers  
- Specific protections for legal research queries
- Detailed information about how user research data is handled

**Recommendation**: Our class will have an opportunity to discuss midpage with one of its principals. This will provide an opportunity to gain clarification (and possibly suggest additions to their website).

[^5]: Free.Law has some [proof of concept](https://free.law/2024/04/16/citation-lookup-api) ideas about using it as a hallucination checker. Professor Chandler is working vigorously on a tool that uses Free.law and CourtListener as an (incomplete) oracle for automated fact checking.
